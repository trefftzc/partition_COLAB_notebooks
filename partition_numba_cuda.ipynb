{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPBapWaS2DQUJ6MWXoC4cEQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trefftzc/partition_COLAB_notebooks/blob/main/partition_numba_cuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -iname 'libdevice'\n",
        "!find / -iname 'libnvvm.so'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc5GNxrViRcE",
        "outputId": "41add8c4-802e-43ab-bde7-7e9b0986ddd1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.julia/artifacts/6283886c5dd600750885a23bff8d37f687172633/share/libdevice\n",
            "/usr/local/lib/python3.12/dist-packages/nvidia/cuda_nvcc/nvvm/libdevice\n",
            "/usr/local/cuda-12.5/nvvm/libdevice\n",
            "find: ‘/proc/62/task/62/net’: Invalid argument\n",
            "find: ‘/proc/62/net’: Invalid argument\n",
            "/usr/local/lib/python3.12/dist-packages/nvidia/cuda_nvcc/nvvm/lib64/libnvvm.so\n",
            "/usr/local/cuda-12.5/nvvm/lib64/libnvvm.so\n",
            "find: ‘/proc/62/task/62/net’: Invalid argument\n",
            "find: ‘/proc/62/net’: Invalid argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['NUMBAPRO_LIBDEVICE'] = \"/usr/local/cuda-12.5/nvvm/libdevice\"\n",
        "os.environ['NUMBAPRO_NVVM'] = \"/usr/local/cuda-12.5/nvvm/lib64/libnvvm.so\""
      ],
      "metadata": {
        "id": "MPht5tLiiVBF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install -q --system numba-cuda==0.15"
      ],
      "metadata": {
        "id": "v5UreyNGiYF0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partition with NUMBA/CUDA\n",
        "This python program is based on NUMBA/CUDA.\n",
        "It solves the partition problem.\n",
        "\n",
        "Make sure that COLAB has been set up to use a GPU.\n",
        "In the main menu select the option:\n",
        " Runtime\n",
        "In the pull-down menu select:\n",
        " Change runtime type\n",
        "Select:\n",
        " T4 GPU"
      ],
      "metadata": {
        "id": "TvlzT8w3e-P6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU cards are dedicated computers with their own memory and their own processors.\n",
        "\n",
        "When programming a GPU, one needs to\n",
        "- allocate memory on the GPU card\n",
        "- copy data from the host's memory to the GPU's memory\n",
        "- execute the kernel, the code that is applied across all the elements of an array\n",
        "- copy the results of interest back to the host\n",
        "\n",
        "This is achieved in the code below in this portion of the code in the function\n",
        " parallelFor\n",
        "\n",
        " arrayGPU = cupy.asarray(array)\n",
        "\n",
        "resultGPU = cupy.asarray(result)\n",
        "\n",
        "  \n",
        "  \n",
        " evaluatePartition[nPartitions//256,256](arrayGPU,resultGPU,n)\n",
        "  \n",
        "  \n",
        "\n",
        "One uses the decorator\n",
        " @cuda.jit\n",
        "to indicate to the numba/cuda compiler the code for the kernel.  \n",
        "\n"
      ],
      "metadata": {
        "id": "55bVlOe8hulr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KPmcXCre9UJ",
        "outputId": "2a48bc27-0d2f-406c-f3ca-4d36f412d796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing partition_numba_cuda.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile partition_numba_cuda.py\n",
        "#\n",
        "# Program that solves the partition problem in python\n",
        "# Parallel version with numba\n",
        "#\n",
        "import sys\n",
        "import numpy as np\n",
        "#import numba\n",
        "from numba import cuda\n",
        "#from numba.cuda.cudadrv.devicearray import DeviceNDArray\n",
        "import cupy\n",
        "import time\n",
        "\n",
        "# A reduction on the GPU\n",
        "@cuda.reduce\n",
        "def max_reduce(a, b):\n",
        "    if a < b:\n",
        "        return b\n",
        "    else:\n",
        "        return a\n",
        "\n",
        "#\n",
        "# This is the kernel, the code that is executed in each processor\n",
        "# in the GPU\n",
        "#\n",
        "#def evaluatePartition(array:DeviceNDArray,result:DeviceNDArray,n:np.dtype=np.int64):\n",
        "@cuda.jit\n",
        "def evaluatePartition(array,result,n):\n",
        "   value = cuda.grid(1)\n",
        "   sum0s = 0\n",
        "   sum1s = 0\n",
        "   mask = 1\n",
        "   for i in range(0,n):\n",
        "    if ((mask & value) != 0):\n",
        "      sum1s = sum1s + array[i]\n",
        "    else:\n",
        "      sum0s = sum0s + array[i]\n",
        "    mask = mask * 2\n",
        "   if (sum0s == sum1s):\n",
        "     # print(\"Evaluate partition \",value,\" returns \",value)\n",
        "     result[value] = value\n",
        "   else:\n",
        "    # print(\"Evaluate partition \",value,\" returns \",0)\n",
        "    result[value] = 0\n",
        "\n",
        "def printResults(value, n, array):\n",
        "  print(\"Solution:\\n\")\n",
        "  print(\"First partition: \")\n",
        "  mask = 1\n",
        "  sum = 0\n",
        "  for i in range(0,n):\n",
        "    if ((mask & value) != 0):\n",
        "      print(array[i],end=\" \")\n",
        "      sum = sum + array[i]\n",
        "    mask = mask * 2\n",
        "  print(\" sum: \",sum)\n",
        "  print(\"Second partition: \")\n",
        "  mask = 1\n",
        "  sum = 0\n",
        "  for i in range(0,n):\n",
        "    if ((mask & value) == 0):\n",
        "      print(array[i],end=\" \")\n",
        "      sum = sum + array[i]\n",
        "    mask = mask * 2\n",
        "  print(\" sum: \\n\",sum)\n",
        "\n",
        "def parallelFor(n,array,nPartitions):\n",
        "  solutionFound = 0\n",
        "  solution = -1\n",
        "  result = np.zeros(nPartitions,dtype=np.int64)\n",
        "  #arrayGPU = cuda.to_device(array)\n",
        "  #resultGPU = cuda.to_device(result)\n",
        "  arrayGPU = cupy.asarray(array)\n",
        "  resultGPU = cupy.asarray(result)\n",
        "  #evaluatePartition.forall(nPartitions)( arrayGPU,resultGPU, n)\n",
        "  evaluatePartition[nPartitions//256,256](arrayGPU,resultGPU,n)\n",
        "  # Perform a reduction to find the maximum value\n",
        "  solutionFound = max_reduce(resultGPU,init=0)\n",
        "  # resultGPU.copy_to_host(result)\n",
        "  # solutionFound = np.max(result)\n",
        "\n",
        "  return solutionFound\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  # Read the problem\n",
        "  n = int(input())\n",
        "  valuesString = input()\n",
        "  values = valuesString.split()\n",
        "  for i in range(len(values)):\n",
        "    values[i] = int(values[i])\n",
        "# Print the instance of the problem\n",
        "  print(\"Problem size: \",n)\n",
        "  print(\"Problem instance: \",values)\n",
        "  nPartitions = 2 ** n\n",
        "  np_array = np.array(values)\n",
        "  # Call twice so that the compilation time is not included\n",
        "  solutionFound = parallelFor(n,np_array,nPartitions)\n",
        "  start = time.time()\n",
        "  solutionFound = parallelFor(n,np_array,nPartitions)\n",
        "  end = time.time()\n",
        "  elapsed = end - start\n",
        "  if (solutionFound):\n",
        "    printResults(solutionFound, n, values)\n",
        "  else:\n",
        "    print(\"No solution was found.\")\n",
        "  print(\"The program took: \",elapsed,\" seconds.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D8xF3hwesoPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's execute the code with several test cases."
      ],
      "metadata": {
        "id": "wj2ywue6sJua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile instanceNoSolution24.Text\n",
        "24\n",
        "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 1000000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX2kQ_uvfrgh",
        "outputId": "086114c4-227a-47de-c0d7-9063bb7a18a3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing instanceNoSolution24.Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python partition_numba_cuda.py < instanceNoSolution24.Text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuDIsNWEgQAy",
        "outputId": "593a4ea3-3eed-4db1-89f2-d4f13efe3e95"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem size:  24\n",
            "Problem instance:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 1000000]\n",
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:687: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:687: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "No solution was found.\n",
            "The program took:  0.06231880187988281  seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test27.Text\n",
        "27\n",
        "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_qEwwgHpInD",
        "outputId": "95888967-12df-4c91-874c-42c3082620b7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test27.Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python partition_numba_cuda.py < test27.Text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yZnV3oCpNdr",
        "outputId": "51acbc1e-1c3d-4f94-c555-56e19acff08b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem size:  27\n",
            "Problem instance:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:687: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:687: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Solution:\n",
            "\n",
            "First partition: \n",
            "1 20 21 22 23 24 25 26 27  sum:  189\n",
            "Second partition: \n",
            "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  sum: \n",
            " 189\n",
            "The program took:  0.5470447540283203  seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test28.Text\n",
        "28\n",
        "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPWgzBoFnvgq",
        "outputId": "1bb4f49c-2339-4ff5-c77a-9e7f8a910bdd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test28.Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python partition_numba_cuda.py < test28.Text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrlP7fzvn05p",
        "outputId": "9f60fee4-341e-4610-a69b-7966c49c8c33"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem size:  28\n",
            "Problem instance:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:687: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:687: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Solution:\n",
            "\n",
            "First partition: \n",
            "7 21 22 23 24 25 26 27 28  sum:  203\n",
            "Second partition: \n",
            "1 2 3 4 5 6 8 9 10 11 12 13 14 15 16 17 18 19 20  sum: \n",
            " 203\n",
            "The program took:  0.9284102916717529  seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test29.Text\n",
        "29\n",
        "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR2eHwuhpW4x",
        "outputId": "54a40aee-e8a2-40b9-b4c7-f68e3ee8c2a5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test29.Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python partition_numba_cuda.py < test29.Text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPgYOCxDpcnr",
        "outputId": "f4be4ae1-cdbe-4fc8-f5ca-72a61c8a49e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem size:  29\n",
            "Problem instance:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:687: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:687: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "No solution was found.\n",
            "The program took:  2.0164570808410645  seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test30.Text\n",
        "30\n",
        "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQmTh7nxprCj",
        "outputId": "425be193-2d95-4ed3-b875-6a60915749a6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test30.Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python partition_numba_cuda.py < test30.Text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADkc1Lorpu5L",
        "outputId": "d641c07e-408d-47b3-8e35-4824cacd119d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem size:  30\n",
            "Problem instance:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:687: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:687: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "No solution was found.\n",
            "The program took:  3.66446852684021  seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lscpu"
      ],
      "metadata": {
        "id": "qT1xPX7pAUxM",
        "outputId": "27202457-1bab-445c-88c5-e834b42956aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:            x86_64\n",
            "  CPU op-mode(s):        32-bit, 64-bit\n",
            "  Address sizes:         46 bits physical, 48 bits virtual\n",
            "  Byte Order:            Little Endian\n",
            "CPU(s):                  2\n",
            "  On-line CPU(s) list:   0,1\n",
            "Vendor ID:               GenuineIntel\n",
            "  Model name:            Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "    CPU family:          6\n",
            "    Model:               85\n",
            "    Thread(s) per core:  2\n",
            "    Core(s) per socket:  1\n",
            "    Socket(s):           1\n",
            "    Stepping:            3\n",
            "    BogoMIPS:            4000.29\n",
            "    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clf\n",
            "                         lush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_\n",
            "                         good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fm\n",
            "                         a cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hyp\n",
            "                         ervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsb\n",
            "                         ase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512d\n",
            "                         q rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsave\n",
            "                         c xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Virtualization features: \n",
            "  Hypervisor vendor:     KVM\n",
            "  Virtualization type:   full\n",
            "Caches (sum of all):     \n",
            "  L1d:                   32 KiB (1 instance)\n",
            "  L1i:                   32 KiB (1 instance)\n",
            "  L2:                    1 MiB (1 instance)\n",
            "  L3:                    38.5 MiB (1 instance)\n",
            "NUMA:                    \n",
            "  NUMA node(s):          1\n",
            "  NUMA node0 CPU(s):     0,1\n",
            "Vulnerabilities:         \n",
            "  Gather data sampling:  Not affected\n",
            "  Itlb multihit:         Not affected\n",
            "  L1tf:                  Mitigation; PTE Inversion\n",
            "  Mds:                   Vulnerable; SMT Host state unknown\n",
            "  Meltdown:              Vulnerable\n",
            "  Mmio stale data:       Vulnerable\n",
            "  Retbleed:              Vulnerable\n",
            "  Spec rstack overflow:  Not affected\n",
            "  Spec store bypass:     Vulnerable\n",
            "  Spectre v1:            Vulnerable: __user pointer sanitization and usercopy barriers only; no swap\n",
            "                         gs barriers\n",
            "  Spectre v2:            Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected\n",
            "  Srbds:                 Not affected\n",
            "  Tsx async abort:       Vulnerable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Fp1Ae8JYAmq1",
        "outputId": "18b7cb28-2465-433b-e916-640ec0a16442",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Sep  4 15:17:45 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile instanceNoSolution25.Text\n",
        "25\n",
        "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 1000000"
      ],
      "metadata": {
        "id": "fu0K4otgTqen",
        "outputId": "7b16d8bc-efe6-4503-9f2a-7732c6e58078",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing instanceNoSolution25.Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python partition_numba_cuda.py < instanceNoSolution25.Text"
      ],
      "metadata": {
        "id": "F0CrLqdOTv5P",
        "outputId": "f3e88b03-a181-4e64-fcb2-3603cbea1344",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem size:  25\n",
            "Problem instance:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 1000000]\n",
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:687: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.12/dist-packages/numba_cuda/numba/cuda/dispatcher.py:687: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "No solution was found.\n",
            "The program took:  0.1255781650543213  seconds.\n"
          ]
        }
      ]
    }
  ]
}