{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNhpa6qeyqlBeI5gmC0ndv5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trefftzc/partition_COLAB_notebooks/blob/main/partition_numba_cuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partition with NUMBA/CUDA\n",
        "This python program is based on NUMBA/CUDA.\n",
        "It solves the partition problem.\n",
        "\n",
        "Make sure that COLAB has been set up to use a GPU.\n",
        "In the main menu select the option:\n",
        " Runtime\n",
        "In the pull-down menu select:\n",
        " Change runtime type\n",
        "Select:\n",
        " T4 GPU"
      ],
      "metadata": {
        "id": "TvlzT8w3e-P6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU cards are dedicated computers with their own memory and their own processors.\n",
        "\n",
        "When programming a GPU, one needs to\n",
        "- allocate memory on the GPU card\n",
        "- copy data from the host's memory to the GPU's memory\n",
        "- execute the kernel, the code that is applied across all the elements of an array\n",
        "- copy the results of interest back to the host\n",
        "\n",
        "This is achieved in the code below in this portion of the code in the function\n",
        " parallelFor\n",
        "\n",
        "  arrayGPU = cuda.to_device(array)\n",
        "\n",
        "  resultGPU = cuda.to_device(result)\n",
        "  \n",
        "  evaluatePartition.forall(nPartitions)( arrayGPU,resultGPU, n)\n",
        "  \n",
        "  resultGPU.copy_to_host(result)\n",
        "\n",
        "One uses the decorator\n",
        " @cuda.jit\n",
        "to indicate to the numba/cuda compiler the code for the kernel.  \n",
        "\n"
      ],
      "metadata": {
        "id": "55bVlOe8hulr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KPmcXCre9UJ",
        "outputId": "2788422b-4f72-43a5-a16e-8bf62911fd60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting partition_numba_cuda.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile partition_numba_cuda.py\n",
        "#\n",
        "# Program that solves the partition problem in python\n",
        "# Parallel version with numba\n",
        "#\n",
        "import sys\n",
        "import numpy as np\n",
        "#import numba\n",
        "from numba import cuda\n",
        "from numba.cuda.cudadrv.devicearray import DeviceNDArray\n",
        "import time\n",
        "\n",
        "# A reduction on the GPU\n",
        "@cuda.reduce\n",
        "def max_reduce(a, b):\n",
        "    if a < b:\n",
        "        return b\n",
        "    else:\n",
        "        return a\n",
        "\n",
        "#\n",
        "# This is the kernel, the code that is executed in each processor\n",
        "# in the GPU\n",
        "#\n",
        "@cuda.jit\n",
        "def evaluatePartition(array:DeviceNDArray,result:DeviceNDArray,n:np.dtype=np.int64):\n",
        "   value = cuda.grid(1)\n",
        "   sum0s = 0\n",
        "   sum1s = 0\n",
        "   mask = 1\n",
        "   for i in range(0,n):\n",
        "    if ((mask & value) != 0):\n",
        "      sum1s = sum1s + array[i]\n",
        "    else:\n",
        "      sum0s = sum0s + array[i]\n",
        "    mask = mask * 2\n",
        "   if (sum0s == sum1s):\n",
        "     # print(\"Evaluate partition \",value,\" returns \",value)\n",
        "     result[value] = value\n",
        "   else:\n",
        "    # print(\"Evaluate partition \",value,\" returns \",0)\n",
        "    result[value] = 0\n",
        "\n",
        "def printResults(value, n, array):\n",
        "  print(\"Solution:\\n\")\n",
        "  print(\"First partition: \")\n",
        "  mask = 1\n",
        "  sum = 0\n",
        "  for i in range(0,n):\n",
        "    if ((mask & value) != 0):\n",
        "      print(array[i],end=\" \")\n",
        "      sum = sum + array[i]\n",
        "    mask = mask * 2\n",
        "  print(\" sum: \",sum)\n",
        "  print(\"Second partition: \")\n",
        "  mask = 1\n",
        "  sum = 0\n",
        "  for i in range(0,n):\n",
        "    if ((mask & value) == 0):\n",
        "      print(array[i],end=\" \")\n",
        "      sum = sum + array[i]\n",
        "    mask = mask * 2\n",
        "  print(\" sum: \\n\",sum)\n",
        "\n",
        "def parallelFor(n,array,nPartitions):\n",
        "  solutionFound = 0\n",
        "  solution = -1\n",
        "  result = np.zeros(nPartitions,dtype=np.int64)\n",
        "  arrayGPU = cuda.to_device(array)\n",
        "  resultGPU = cuda.to_device(result)\n",
        "  evaluatePartition.forall(nPartitions)( arrayGPU,resultGPU, n)\n",
        "  # Copy the result array back to the CPU\n",
        "  solutionFound = max_reduce(resultGPU,init=0)\n",
        "  # resultGPU.copy_to_host(result)\n",
        "  # solutionFound = np.max(result)\n",
        "  solution = solutionFound\n",
        "  if (solutionFound):\n",
        "    printResults(solution, n, array)\n",
        "  else:\n",
        "    print(\"No solution was found.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  start = time.time()\n",
        "  # Read the problem\n",
        "  n = int(input())\n",
        "  valuesString = input()\n",
        "  values = valuesString.split()\n",
        "  for i in range(len(values)):\n",
        "    values[i] = int(values[i])\n",
        "# Print the instance of the problem\n",
        "  print(\"Problem size: \",n)\n",
        "  print(\"Problem instance: \",values)\n",
        "  nPartitions = 2 ** n\n",
        "  np_array = np.array(values)\n",
        "  parallelFor(n,np_array,nPartitions)\n",
        "  end = time.time()\n",
        "  elapsed = end - start\n",
        "  print(\"The program took: \",elapsed,\" seconds.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D8xF3hwesoPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's execute the code with several test cases."
      ],
      "metadata": {
        "id": "wj2ywue6sJua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile instanceNoSolution24.Text\n",
        "24\n",
        "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 1000000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX2kQ_uvfrgh",
        "outputId": "9d6a7035-3aa5-46b9-fd23-85237cda19f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing instanceNoSolution24.Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python partition_numba_cuda.py < instanceNoSolution24.Text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuDIsNWEgQAy",
        "outputId": "a33d4536-d385-4fb8-a164-92e4688e2240"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem size:  24\n",
            "Problem instance:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 1000000]\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "No solution was found.\n",
            "The program took:  0.9093437194824219  seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test27.Text\n",
        "27\n",
        "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_qEwwgHpInD",
        "outputId": "68c2c387-ed82-4136-94d4-e40f0da54922"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test27.Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python partition_numba_cuda.py < test27.Text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yZnV3oCpNdr",
        "outputId": "71418f94-a93d-47c9-cb05-c889ec982900"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem size:  27\n",
            "Problem instance:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Solution:\n",
            "\n",
            "First partition: \n",
            "1 20 21 22 23 24 25 26 27  sum:  189\n",
            "Second partition: \n",
            "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  sum: \n",
            " 189\n",
            "The program took:  1.2284221649169922  seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test28.Text\n",
        "28\n",
        "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPWgzBoFnvgq",
        "outputId": "548da84e-d2d5-4ca4-def1-5f44eec986e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test28.Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python partition_numba_cuda.py < test28.Text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrlP7fzvn05p",
        "outputId": "5cb1a485-7020-403a-a311-1e97f2125d6b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem size:  28\n",
            "Problem instance:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Solution:\n",
            "\n",
            "First partition: \n",
            "7  \n",
            "21  \n",
            "22  \n",
            "23  \n",
            "24  \n",
            "25  \n",
            "26  \n",
            "27  \n",
            "28  \n",
            " sum:  203\n",
            "Second partition: \n",
            "1  \n",
            "2  \n",
            "3  \n",
            "4  \n",
            "5  \n",
            "6  \n",
            "8  \n",
            "9  \n",
            "10  \n",
            "11  \n",
            "12  \n",
            "13  \n",
            "14  \n",
            "15  \n",
            "16  \n",
            "17  \n",
            "18  \n",
            "19  \n",
            "20  \n",
            " sum: \n",
            " 203\n",
            "The program took:  1.564784049987793  seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test29.Text\n",
        "29\n",
        "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR2eHwuhpW4x",
        "outputId": "849b8a95-70be-4b74-85da-a254165abefa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test29.Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python partition_numba_cuda.py < test29.Text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPgYOCxDpcnr",
        "outputId": "53533b4a-d09a-4640-fe1f-29be57170ad2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem size:  29\n",
            "Problem instance:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "No solution was found.\n",
            "The program took:  2.2813358306884766  seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test30.Text\n",
        "30\n",
        "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQmTh7nxprCj",
        "outputId": "10ad9f59-8f3c-4515-c17c-2c59e600fec0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test30.Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python partition_numba_cuda.py < test30.Text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADkc1Lorpu5L",
        "outputId": "e9c9e728-df32-4c34-fe84-bca45f3a7424"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem size:  30\n",
            "Problem instance:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "No solution was found.\n",
            "The program took:  3.810377359390259  seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lscpu"
      ],
      "metadata": {
        "id": "qT1xPX7pAUxM",
        "outputId": "27202457-1bab-445c-88c5-e834b42956aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:            x86_64\n",
            "  CPU op-mode(s):        32-bit, 64-bit\n",
            "  Address sizes:         46 bits physical, 48 bits virtual\n",
            "  Byte Order:            Little Endian\n",
            "CPU(s):                  2\n",
            "  On-line CPU(s) list:   0,1\n",
            "Vendor ID:               GenuineIntel\n",
            "  Model name:            Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "    CPU family:          6\n",
            "    Model:               85\n",
            "    Thread(s) per core:  2\n",
            "    Core(s) per socket:  1\n",
            "    Socket(s):           1\n",
            "    Stepping:            3\n",
            "    BogoMIPS:            4000.29\n",
            "    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clf\n",
            "                         lush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_\n",
            "                         good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fm\n",
            "                         a cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hyp\n",
            "                         ervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsb\n",
            "                         ase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512d\n",
            "                         q rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsave\n",
            "                         c xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Virtualization features: \n",
            "  Hypervisor vendor:     KVM\n",
            "  Virtualization type:   full\n",
            "Caches (sum of all):     \n",
            "  L1d:                   32 KiB (1 instance)\n",
            "  L1i:                   32 KiB (1 instance)\n",
            "  L2:                    1 MiB (1 instance)\n",
            "  L3:                    38.5 MiB (1 instance)\n",
            "NUMA:                    \n",
            "  NUMA node(s):          1\n",
            "  NUMA node0 CPU(s):     0,1\n",
            "Vulnerabilities:         \n",
            "  Gather data sampling:  Not affected\n",
            "  Itlb multihit:         Not affected\n",
            "  L1tf:                  Mitigation; PTE Inversion\n",
            "  Mds:                   Vulnerable; SMT Host state unknown\n",
            "  Meltdown:              Vulnerable\n",
            "  Mmio stale data:       Vulnerable\n",
            "  Retbleed:              Vulnerable\n",
            "  Spec rstack overflow:  Not affected\n",
            "  Spec store bypass:     Vulnerable\n",
            "  Spectre v1:            Vulnerable: __user pointer sanitization and usercopy barriers only; no swap\n",
            "                         gs barriers\n",
            "  Spectre v2:            Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected\n",
            "  Srbds:                 Not affected\n",
            "  Tsx async abort:       Vulnerable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Fp1Ae8JYAmq1",
        "outputId": "9049ee8a-e053-49ca-ec10-1e937ee31292",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan 31 16:17:52 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P0              35W /  70W |      0MiB / 15360MiB |     50%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile instanceNoSolution25.Text\n",
        "25\n",
        "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 1000000"
      ],
      "metadata": {
        "id": "fu0K4otgTqen",
        "outputId": "792594c7-831e-45fa-dee3-2768f4134fc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing instanceNoSolution25.Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python partition_numba_cuda.py < instanceNoSolution25.Text"
      ],
      "metadata": {
        "id": "F0CrLqdOTv5P",
        "outputId": "6b02ee90-cb96-412f-cffb-aa0db490df47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem size:  25\n",
            "Problem instance:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 1000000]\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "No solution was found.\n",
            "The program took:  0.9873096942901611  seconds.\n"
          ]
        }
      ]
    }
  ]
}