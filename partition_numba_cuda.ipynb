{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMrGdgQmkQQpXMw3yd7gPyA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trefftzc/partition_COLAB_notebooks/blob/main/partition_numba_cuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -iname 'libdevice'\n",
        "!find / -iname 'libnvvm.so'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc5GNxrViRcE",
        "outputId": "6355bade-80f3-4a03-d2b6-05a66ed36e14"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nvidia/cuda_nvcc/nvvm/libdevice\n",
            "/usr/local/cuda-12.5/nvvm/libdevice\n",
            "find: ‘/proc/522’: No such file or directory\n",
            "/usr/local/lib/python3.11/dist-packages/nvidia/cuda_nvcc/nvvm/lib64/libnvvm.so\n",
            "/usr/local/cuda-12.5/nvvm/lib64/libnvvm.so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['NUMBAPRO_LIBDEVICE'] = \"/usr/local/cuda-12.5/nvvm/libdevice\"\n",
        "os.environ['NUMBAPRO_NVVM'] = \"/usr/local/cuda-12.5/nvvm/lib64/libnvvm.so\""
      ],
      "metadata": {
        "id": "MPht5tLiiVBF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install -q --system numba-cuda==0.4.0"
      ],
      "metadata": {
        "id": "v5UreyNGiYF0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numba import config\n",
        "config.CUDA_ENABLE_PYNVJITLINK = 1"
      ],
      "metadata": {
        "id": "_tytcbAWiiq3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partition with NUMBA/CUDA\n",
        "This python program is based on NUMBA/CUDA.\n",
        "It solves the partition problem.\n",
        "\n",
        "Make sure that COLAB has been set up to use a GPU.\n",
        "In the main menu select the option:\n",
        " Runtime\n",
        "In the pull-down menu select:\n",
        " Change runtime type\n",
        "Select:\n",
        " T4 GPU"
      ],
      "metadata": {
        "id": "TvlzT8w3e-P6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU cards are dedicated computers with their own memory and their own processors.\n",
        "\n",
        "When programming a GPU, one needs to\n",
        "- allocate memory on the GPU card\n",
        "- copy data from the host's memory to the GPU's memory\n",
        "- execute the kernel, the code that is applied across all the elements of an array\n",
        "- copy the results of interest back to the host\n",
        "\n",
        "This is achieved in the code below in this portion of the code in the function\n",
        " parallelFor\n",
        "\n",
        "  arrayGPU = cuda.to_device(array)\n",
        "\n",
        "  resultGPU = cuda.to_device(result)\n",
        "  \n",
        "  evaluatePartition.forall(nPartitions)( arrayGPU,resultGPU, n)\n",
        "  \n",
        "  resultGPU.copy_to_host(result)\n",
        "\n",
        "One uses the decorator\n",
        " @cuda.jit\n",
        "to indicate to the numba/cuda compiler the code for the kernel.  \n",
        "\n"
      ],
      "metadata": {
        "id": "55bVlOe8hulr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KPmcXCre9UJ",
        "outputId": "3cd6e850-6ab6-49b3-9499-c5a3d383910e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting partition_numba_cuda.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile partition_numba_cuda.py\n",
        "#\n",
        "# Program that solves the partition problem in python\n",
        "# Parallel version with numba\n",
        "#\n",
        "import sys\n",
        "import numpy as np\n",
        "#import numba\n",
        "from numba import cuda\n",
        "#from numba.cuda.cudadrv.devicearray import DeviceNDArray\n",
        "import cupy\n",
        "import time\n",
        "\n",
        "# A reduction on the GPU\n",
        "@cuda.reduce\n",
        "def max_reduce(a, b):\n",
        "    if a < b:\n",
        "        return b\n",
        "    else:\n",
        "        return a\n",
        "\n",
        "#\n",
        "# This is the kernel, the code that is executed in each processor\n",
        "# in the GPU\n",
        "#\n",
        "@cuda.jit\n",
        "#def evaluatePartition(array:DeviceNDArray,result:DeviceNDArray,n:np.dtype=np.int64):\n",
        "def evaluatePartition(array,result,n):\n",
        "   value = cuda.grid(1)\n",
        "   sum0s = 0\n",
        "   sum1s = 0\n",
        "   mask = 1\n",
        "   for i in range(0,n):\n",
        "    if ((mask & value) != 0):\n",
        "      sum1s = sum1s + array[i]\n",
        "    else:\n",
        "      sum0s = sum0s + array[i]\n",
        "    mask = mask * 2\n",
        "   if (sum0s == sum1s):\n",
        "     # print(\"Evaluate partition \",value,\" returns \",value)\n",
        "     result[value] = value\n",
        "   else:\n",
        "    # print(\"Evaluate partition \",value,\" returns \",0)\n",
        "    result[value] = 0\n",
        "\n",
        "def printResults(value, n, array):\n",
        "  print(\"Solution:\\n\")\n",
        "  print(\"First partition: \")\n",
        "  mask = 1\n",
        "  sum = 0\n",
        "  for i in range(0,n):\n",
        "    if ((mask & value) != 0):\n",
        "      print(array[i],end=\" \")\n",
        "      sum = sum + array[i]\n",
        "    mask = mask * 2\n",
        "  print(\" sum: \",sum)\n",
        "  print(\"Second partition: \")\n",
        "  mask = 1\n",
        "  sum = 0\n",
        "  for i in range(0,n):\n",
        "    if ((mask & value) == 0):\n",
        "      print(array[i],end=\" \")\n",
        "      sum = sum + array[i]\n",
        "    mask = mask * 2\n",
        "  print(\" sum: \\n\",sum)\n",
        "\n",
        "def parallelFor(n,array,nPartitions):\n",
        "  solutionFound = 0\n",
        "  solution = -1\n",
        "  result = np.zeros(nPartitions,dtype=np.int64)\n",
        "  #arrayGPU = cuda.to_device(array)\n",
        "  #resultGPU = cuda.to_device(result)\n",
        "  arrayGPU = cupy.asarray(array)\n",
        "  resultGPU = cupy.asarray(result)\n",
        "  #evaluatePartition.forall(nPartitions)( arrayGPU,resultGPU, n)\n",
        "  evaluatePartition[nPartitions//256,256](arrayGPU,resultGPU,n)\n",
        "  # Perform a reduction to find the maximum value\n",
        "  solutionFound = max_reduce(resultGPU,init=0)\n",
        "  # resultGPU.copy_to_host(result)\n",
        "  # solutionFound = np.max(result)\n",
        "\n",
        "  return solutionFound\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  # Read the problem\n",
        "  n = int(input())\n",
        "  valuesString = input()\n",
        "  values = valuesString.split()\n",
        "  for i in range(len(values)):\n",
        "    values[i] = int(values[i])\n",
        "# Print the instance of the problem\n",
        "  print(\"Problem size: \",n)\n",
        "  print(\"Problem instance: \",values)\n",
        "  nPartitions = 2 ** n\n",
        "  np_array = np.array(values)\n",
        "  # Call twice so that the compilation time is not included\n",
        "  solutionFound = parallelFor(n,np_array,nPartitions)\n",
        "  start = time.time()\n",
        "  solutionFound = parallelFor(n,np_array,nPartitions)\n",
        "  end = time.time()\n",
        "  elapsed = end - start\n",
        "  if (solutionFound):\n",
        "    printResults(solutionFound, n, values)\n",
        "  else:\n",
        "    print(\"No solution was found.\")\n",
        "  print(\"The program took: \",elapsed,\" seconds.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D8xF3hwesoPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's execute the code with several test cases."
      ],
      "metadata": {
        "id": "wj2ywue6sJua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile instanceNoSolution24.Text\n",
        "24\n",
        "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 1000000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX2kQ_uvfrgh",
        "outputId": "4e7077cd-008a-43c2-9aaa-2f991ba04fc5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing instanceNoSolution24.Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python partition_numba_cuda.py < instanceNoSolution24.Text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuDIsNWEgQAy",
        "outputId": "494dd667-455a-4ba6-de0a-00a4fdb398d7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem size:  24\n",
            "Problem instance:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 1000000]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\", line 2901, in add_ptx\n",
            "    driver.cuLinkAddData(self.handle, enums.CU_JIT_INPUT_PTX,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\", line 327, in safe_cuda_api_call\n",
            "    self._check_ctypes_error(fname, retcode)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\", line 395, in _check_ctypes_error\n",
            "    raise CudaAPIError(retcode, msg)\n",
            "numba.cuda.cudadrv.driver.CudaAPIError: [222] Call to cuLinkAddData results in CUDA_ERROR_UNSUPPORTED_PTX_VERSION\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/partition_numba_cuda.py\", line 97, in <module>\n",
            "    solutionFound = parallelFor(n,np_array,nPartitions)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/partition_numba_cuda.py\", line 75, in parallelFor\n",
            "    evaluatePartition[nPartitions//256,256](arrayGPU,resultGPU,n)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py\", line 608, in __call__\n",
            "    return self.dispatcher.call(args, self.griddim, self.blockdim,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py\", line 750, in call\n",
            "    kernel = _dispatcher.Dispatcher._cuda_call(self, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py\", line 758, in _compile_for_args\n",
            "    return self.compile(tuple(argtypes))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py\", line 1005, in compile\n",
            "    kernel.bind()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/dispatcher.py\", line 257, in bind\n",
            "    cufunc = self._codelibrary.get_cufunc()\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/codegen.py\", line 248, in get_cufunc\n",
            "    cubin = self.get_cubin(cc=device.compute_capability)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/codegen.py\", line 227, in get_cubin\n",
            "    self._link_all(linker, cc, ignore_nonlto=False)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/codegen.py\", line 189, in _link_all\n",
            "    linker.add_ptx(ptx.encode())\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\", line 2904, in add_ptx\n",
            "    raise LinkerError(\"%s\\n%s\" % (e, self.error_log))\n",
            "numba.cuda.cudadrv.driver.LinkerError: [222] Call to cuLinkAddData results in CUDA_ERROR_UNSUPPORTED_PTX_VERSION\n",
            "ptxas application ptx input, line 9; fatal   : Unsupported .version 8.5; current version is '8.4'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test27.Text\n",
        "27\n",
        "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_qEwwgHpInD",
        "outputId": "68c2c387-ed82-4136-94d4-e40f0da54922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test27.Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python partition_numba_cuda.py < test27.Text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yZnV3oCpNdr",
        "outputId": "783f485b-860e-4199-8a90-6de8259bfcc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem size:  27\n",
            "Problem instance:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Solution:\n",
            "\n",
            "First partition: \n",
            "1 20 21 22 23 24 25 26 27  sum:  189\n",
            "Second partition: \n",
            "2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  sum: \n",
            " 189\n",
            "The program took:  0.43323230743408203  seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test28.Text\n",
        "28\n",
        "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPWgzBoFnvgq",
        "outputId": "548da84e-d2d5-4ca4-def1-5f44eec986e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test28.Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python partition_numba_cuda.py < test28.Text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrlP7fzvn05p",
        "outputId": "b5b64a67-46d3-45af-d29c-14bb13a66323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem size:  28\n",
            "Problem instance:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Solution:\n",
            "\n",
            "First partition: \n",
            "7 21 22 23 24 25 26 27 28  sum:  203\n",
            "Second partition: \n",
            "1 2 3 4 5 6 8 9 10 11 12 13 14 15 16 17 18 19 20  sum: \n",
            " 203\n",
            "The program took:  0.8493030071258545  seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test29.Text\n",
        "29\n",
        "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR2eHwuhpW4x",
        "outputId": "849b8a95-70be-4b74-85da-a254165abefa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test29.Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python partition_numba_cuda.py < test29.Text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPgYOCxDpcnr",
        "outputId": "48158d1b-341b-46cd-bf0c-4cd1c29c7811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem size:  29\n",
            "Problem instance:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "No solution was found.\n",
            "The program took:  1.60410737991333  seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile test30.Text\n",
        "30\n",
        "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQmTh7nxprCj",
        "outputId": "10ad9f59-8f3c-4515-c17c-2c59e600fec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test30.Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python partition_numba_cuda.py < test30.Text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADkc1Lorpu5L",
        "outputId": "a24174d0-5ec8-49f7-f898-c24e12ec9d14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem size:  30\n",
            "Problem instance:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/driver.py\", line 851, in _attempt_allocation\n",
            "    return allocator()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/driver.py\", line 1062, in allocator\n",
            "    driver.cuMemAlloc(byref(ptr), size)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/driver.py\", line 327, in safe_cuda_api_call\n",
            "    self._check_ctypes_error(fname, retcode)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/driver.py\", line 395, in _check_ctypes_error\n",
            "    raise CudaAPIError(retcode, msg)\n",
            "numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuMemAlloc results in CUDA_ERROR_OUT_OF_MEMORY\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/partition_numba_cuda.py\", line 94, in <module>\n",
            "    solutionFound = parallelFor(n,np_array,nPartitions)\n",
            "  File \"/content/partition_numba_cuda.py\", line 69, in parallelFor\n",
            "    resultGPU = cuda.to_device(result)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devices.py\", line 232, in _require_cuda_context\n",
            "    return fn(*args, **kws)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/api.py\", line 128, in to_device\n",
            "    to, new = devicearray.auto_device(obj, stream=stream, copy=copy,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py\", line 878, in auto_device\n",
            "    devobj = from_array_like(obj, stream=stream)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py\", line 799, in from_array_like\n",
            "    return DeviceNDArray(ary.shape, ary.strides, ary.dtype, stream=stream,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/devicearray.py\", line 104, in __init__\n",
            "    gpu_data = devices.get_context().memalloc(self.alloc_size)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/driver.py\", line 1372, in memalloc\n",
            "    return self.memory_manager.memalloc(bytesize)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/driver.py\", line 1064, in memalloc\n",
            "    self._attempt_allocation(allocator)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/driver.py\", line 863, in _attempt_allocation\n",
            "    return allocator()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/driver.py\", line 1062, in allocator\n",
            "    driver.cuMemAlloc(byref(ptr), size)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/driver.py\", line 327, in safe_cuda_api_call\n",
            "    self._check_ctypes_error(fname, retcode)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/driver.py\", line 395, in _check_ctypes_error\n",
            "    raise CudaAPIError(retcode, msg)\n",
            "numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuMemAlloc results in CUDA_ERROR_OUT_OF_MEMORY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lscpu"
      ],
      "metadata": {
        "id": "qT1xPX7pAUxM",
        "outputId": "27202457-1bab-445c-88c5-e834b42956aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:            x86_64\n",
            "  CPU op-mode(s):        32-bit, 64-bit\n",
            "  Address sizes:         46 bits physical, 48 bits virtual\n",
            "  Byte Order:            Little Endian\n",
            "CPU(s):                  2\n",
            "  On-line CPU(s) list:   0,1\n",
            "Vendor ID:               GenuineIntel\n",
            "  Model name:            Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "    CPU family:          6\n",
            "    Model:               85\n",
            "    Thread(s) per core:  2\n",
            "    Core(s) per socket:  1\n",
            "    Socket(s):           1\n",
            "    Stepping:            3\n",
            "    BogoMIPS:            4000.29\n",
            "    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clf\n",
            "                         lush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_\n",
            "                         good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fm\n",
            "                         a cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hyp\n",
            "                         ervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsb\n",
            "                         ase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512d\n",
            "                         q rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsave\n",
            "                         c xgetbv1 xsaves arat md_clear arch_capabilities\n",
            "Virtualization features: \n",
            "  Hypervisor vendor:     KVM\n",
            "  Virtualization type:   full\n",
            "Caches (sum of all):     \n",
            "  L1d:                   32 KiB (1 instance)\n",
            "  L1i:                   32 KiB (1 instance)\n",
            "  L2:                    1 MiB (1 instance)\n",
            "  L3:                    38.5 MiB (1 instance)\n",
            "NUMA:                    \n",
            "  NUMA node(s):          1\n",
            "  NUMA node0 CPU(s):     0,1\n",
            "Vulnerabilities:         \n",
            "  Gather data sampling:  Not affected\n",
            "  Itlb multihit:         Not affected\n",
            "  L1tf:                  Mitigation; PTE Inversion\n",
            "  Mds:                   Vulnerable; SMT Host state unknown\n",
            "  Meltdown:              Vulnerable\n",
            "  Mmio stale data:       Vulnerable\n",
            "  Retbleed:              Vulnerable\n",
            "  Spec rstack overflow:  Not affected\n",
            "  Spec store bypass:     Vulnerable\n",
            "  Spectre v1:            Vulnerable: __user pointer sanitization and usercopy barriers only; no swap\n",
            "                         gs barriers\n",
            "  Spectre v2:            Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected\n",
            "  Srbds:                 Not affected\n",
            "  Tsx async abort:       Vulnerable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Fp1Ae8JYAmq1",
        "outputId": "9049ee8a-e053-49ca-ec10-1e937ee31292",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan 31 16:17:52 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P0              35W /  70W |      0MiB / 15360MiB |     50%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile instanceNoSolution25.Text\n",
        "25\n",
        "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 1000000"
      ],
      "metadata": {
        "id": "fu0K4otgTqen",
        "outputId": "792594c7-831e-45fa-dee3-2768f4134fc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing instanceNoSolution25.Text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python partition_numba_cuda.py < instanceNoSolution25.Text"
      ],
      "metadata": {
        "id": "F0CrLqdOTv5P",
        "outputId": "6b02ee90-cb96-412f-cffb-aa0db490df47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem size:  25\n",
            "Problem instance:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 1000000]\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 1 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n",
            "No solution was found.\n",
            "The program took:  0.9873096942901611  seconds.\n"
          ]
        }
      ]
    }
  ]
}